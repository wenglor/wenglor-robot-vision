{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Robot Vision Manual","text":"<p>This manual demonstrates how a general usage of the the robot vision interace with wenglor vision devices can look like.</p>"},{"location":"1_0_0_intended_purpose/","title":"1. Use for Intended Purpose","text":"<p>Machine Vision Devices can communicate directly to robots for robot vision applications (e.g. pick and place applications).</p> <p>Supported Machine Vision Devices:</p> <ul> <li>Smart Camera B60</li> <li>Machine Vision Controller MVC with Machine Vision Cameras BBVK and BBZK</li> </ul> <p>NOTE</p> <p>For details about the Machine Vision Devices check the operating instructions of the device. The interface protocol for Robot Vision addresses users with knowledge in robotic programming and basic knowledge in Machine Vision.</p>"},{"location":"2_0_0_network_overview/","title":"2. Network Overview","text":"<p>Connect the network cable from the LAN port of the Machine Vision Device to the robot controller or to a switch in order to work with several devices.</p> <p>NOTE</p> <ul> <li>For details about the LAN port of the Machine Vision Device, check the operating instructions of the device.</li> <li>Cabling must support 1\u202fGbit/s throughout the entire network.  </li> <li>Split the networks for LAN, RTE and CAM in order to optimize the performance of the Machine Vision Device.</li> <li>Use a unique network configuration for the LAN port of the Machine Vision Device within the LAN network. Furthermore, the network configuration of the LAN port must be different to the network configuration of the RTE and CAM ports of the Machine Vision Device.</li> </ul> <p></p>"},{"location":"3_0_0_robot_vision_basics/","title":"3. Robot Vision Basics","text":"<p>Machine Vision Devices connect to robot controllers through the wenglor robot server. The robot server runs directly on each Machine Vision Device, and its settings can be accessed via the device\u2019s website. For every Processing Instance, you\u2019ll find the robot server available under the \u201cJobs\u201d tab. For further details, please refer to your Machine Vision Device\u2019s operating instructions.</p>"},{"location":"3_0_0_robot_vision_basics/#robot-vision-features","title":"Robot Vision Features","text":"<ul> <li>Communication to supported robot manufacturers (listed in dropdown of Robot Manufacturer on the device website of the Machine Vision Device at the tab \u201cJobs\u201d) . Using <code>Generic</code> in the dropdown allows the communication to further robot manufacturers via the generic string based robot vision API (see chapter 4.5 Generic Robots).</li> <li>Camera on robot or not on robot.</li> <li>If the camera is tilted towards the measuring or picking plane, the calibration compensates for this. Extreme angles should be avoided.</li> <li>Hand-eye calibration of robot and camera via several calibration poses where the camera looks from different positions on the calibration plate (buy calibration plate ZVZJ or print corresponding PDF on flat and stiff material by yourself).</li> <li>Calibration (including elimination of lens distortion and calculating in mm) is done within the robot server (no separate <code>Module Image Calibration</code> within the uniVision job is required).</li> <li>Use <code>Device Robot Vision</code> within the uniVision job for the communication to the robot server</li> </ul>"},{"location":"3_0_0_robot_vision_basics/#robot-related-hints","title":"Robot related hints","text":"<ul> <li>Mount the camera on the robot or not on the robot. For details about mounting options, check the product detail page of the Machine Vision Device on wenglor.com.</li> <li>Make sure to use robot-compatible cables. For details, check the product detail page of the Machine Vision Device.</li> <li>For safe cable management, use the joint limits in the safety settings of the robot.</li> <li>Set the payload on the robot (if necessary).</li> <li>Teach the robot TCP (tool center point) in relation to the robot world coordinate system.</li> </ul> <p>NOTE</p> <p>Make sure that the mechanical setup of camera, robot and picking plane is unchanged after the calibration. Otherwise, re-calibrating is necessary.</p>"},{"location":"5_0_0_change_index_of_operating_instructions/","title":"5. Change Index of Operating Instructions","text":"Version Date Description Associated software version 1.0.0 18.08.2025 Initial version Software uniVision 3.6.0"},{"location":"4_0_robot_vision_server/","title":"4. Wenglor Robot Server","text":"<p>The wenglor robot server supports the communication to specific robot manufacturers. For details, check the dropdown list of robot manufacturers on the device website of the Machine Vision Device and the operating instructions of the device. Using <code>Generic</code> in the dropdown allows the communication to further robot manufacturers via the generic string based robot vision API (see chapter 4.5 Generic Robots).</p> <p>It is possible to mount the camera on the robot or not on the robot. Even slightly tilting of the camera towards the measuring or picking plane is supported.</p> <p></p> <p>Calibrate robot and camera via several calibration poses where the camera looks from different positions on the calibration plate (hand-eye calibration). Buy the calibration plate ZVZJ or print the corresponding PDF on a flat and stiff material. The calibration is done inside of the wenglor robot server including the compensation of the lens distortion and the calculations in mm (Module Image Calibration is not needed within the uniVision job). Use Device Robot Vision within the uniVision job to send results (coordinates of found objects) to the robot server.</p> <p></p>"},{"location":"4_0_robot_vision_server/4_2_0_settings_on_device_website/","title":"4.2 Settings on Device Website","text":"<p>Open the device website of the Machine Vision Device and select the tab <code>Jobs</code> to see the robot server settings for each processing instance.</p> <p></p> <p>By default, the robot server is active on the Smart Camera B60 and inactive on the Machine Vision Controller MVC. Activate the robot server, if needed. Additional parameters appear if activated:</p>"},{"location":"4_0_robot_vision_server/4_2_0_settings_on_device_website/#settings","title":"Settings","text":"<ul> <li>Robot Manufacturer: Select the relevant robot manufacturer (UR Polyscope 5 URCap, UR Polyscope X Script, Kuka, ABB, Kassow or Generic).</li> <li>Robot Port: Depending on the robot manufacturer, the default port is shown. Edit the port, if needed (e.g. in case of port conflicts at several processing instances on the Machine Vision Controller MVC). Make sure to use a unique port. In case the port is already in use, change the port and wait approximately one minute for the error to disappear.</li> <li>Current Calibration File: Shows the name of the currently loaded calibration file.</li> <li>Load Calibration File: Load another calibration file. The calibration file is only valid for one specific device and valid only if the relation between camera, lens and robot is unchanged (e.g. it gets invalid if the camera position is changed or if the lens is changed, e.g. via another focus position at B60 auto-focus devices).</li> </ul>"},{"location":"4_0_robot_vision_server/4_2_0_settings_on_device_website/#status","title":"Status","text":"<ul> <li>Robot Connection: Shows if the robot controller is connected to the robot server or not.</li> <li>Processing Instance Connection: Shows if the robot server is connected to the processing instance via the LIMA Read Write Limited port or not.</li> <li>Device Robot: Shows if the robot server is connected to <code>Device Robot Vision</code> or not.</li> <li>Error: Shows if there is any error (e.g. if calibration file does not fit to the device).</li> </ul>"},{"location":"4_0_robot_vision_server/4_3_0_calibration_job_in_univision/","title":"4.3 Calibration Job in uniVision","text":"<p>Open the device website of the Machine Vision Device, access the tab Jobs and open the current job in the uniVision 3 software.</p> <p></p> <p>Load the template <code>Calibrate camera with robot</code> for easy setup.</p> <p></p> <p>Adjust focus and brightness of the camera to get a sharp and well illuminated image. By default, Trigger Mode is set to <code>On</code> and Trigger Source to <code>Software</code> (mandatory for the calibration procedure). Adjust <code>Trigger Mode</code> to <code>Off</code> and switch to <code>Run Mode</code> in order to adjust the camera image. Afterwards change the setting back to Software trigger. If working with color images, make sure that <code>Create BGRA Image</code> is active at the input camera (only relevant if working with URCap).</p> <p>Make sure that <code>Device Robot Vision</code> with its default settings is part of the uniVision job.</p> <p>Save the calibration job in the device projects folder on the Machine Vision Device.</p>"},{"location":"4_0_robot_vision_server/4_4_0_detection_job_in_univision/","title":"4.4 Detection Job in uniVision","text":"<p>Open the device website of the Machine Vision Device, access the tab Jobs and open the current job in the uniVision 3 software.</p> <p></p> <p>Load the template <code>Pick objects with robot</code> for easy setup.</p> <p></p> <p>Adjust focus and brightness of the camera to get a sharp and well illuminated image. By default, Trigger Mode is set to On and Trigger Source to Software (mandatory for the calibration procedure). Adjust <code>Trigger Mode</code> to <code>Off</code> and switch to <code>Run Mode</code> in order to adjust the camera image. Afterwards change the setting back to Software trigger. If working with color images, make sure that <code>Create BGRA Image</code> is active at the input camera (only relevant if working with URCap).</p> <p>Teach your object at <code>Module Image Locator</code> (or <code>Module Image Pattern Match</code>). For details, check the operating instructions of the software wenglor uniVision 3 (DNNF023).</p> <p></p> <p>In the template, <code>Device Robot Vision</code> is already pre-configured to send the results to the robot server.</p> Process Time \\(\\mu\\)s Process time to run the module in \\(\\mu\\)s Module State Shows state of module: <ul><li> 0: No error </li><li> Diferent to 0: Error </li></ul> Output Shows preview of the output sent to the wenglor robot server. Shape Model Count Defines the number of shape models (object types) used in the submodule <code>Shape Model Height Difference</code> Result Max Count Defines the number of results used in the sub-module <code>Result List</code>. Result True Count Link the value with <code>Result True Count</code> of <code>Module Image Locator</code> or <code>Module Image Pattern Match</code> so that the robot knows how many objects are within the current image. By default, the value is 0. Linking <code>Result True Count</code> is mandatory so that the robot server knows how many objects are found in the current image. <p>Sub-Module <code>Shape Model Height Difference</code> (number of entries depends on parameter <code>Shape Model Count</code>):</p> <p>Height Difference to Calibration [mm] #...x: Defines the height difference of each shape model to the calibration plate. By default, the value is 0.</p> <p>NOTE:</p> <ul> <li>It is possible to pick different object types with different heights.</li> <li>Enter the object height relative to the surface of the calibration plate in mm. If the object is lower than the calibration plate plane, enter a negative number. If the object is higher, enter a positive number. Use the height difference also in case of height variations after the calibration process.</li> </ul> <p>Sub-Module <code>Result List</code> (number of entries depend on parameter <code>Result Max Count</code>)</p> <p>Link the result list of <code>Module Image Locator</code> or <code>Module Image Pattern Match</code> to the Result List of <code>Device Robot Vision</code>.</p> Shape Model Link the <code>Shape Model</code> (object type) for each result. By default, the value is 0. X [px] Link the x position for each result. Linking the x coordinate is madatory. Y [px] Link the y position for each result. Linking the y coordinate is mandatory. Phi (Z-Rotation) [deg] Link the Phi value (z rotation) for each result. Linking Phi is mandatory. Additional Value Optionally link an additional string result (e.g. score value) for each result. <p>NOTE:</p> <ul> <li>It is possible to detect and pick multiple objects in a single image capture (if setup accordingly in the uniVision job).</li> <li>Offsets in x and y can be set directly in <code>Module Image Locator</code> or <code>Module Image Pattern Match</code>.</li> <li>Pixel results for x and y can only be used by the robot if created by modules that use the  original camera image or any image based on it. No support of x and y coordinates created by modules that use the undistorted or perspective transformed image of <code>Module Image Calibration</code> or any image based on it. Exceptionally, it is supported to use the undistorted image in <code>Module Image Locator</code> or in <code>Module Image Pattern Match</code> with linked input calibration if linking the distorted pixel coordinates at <code>Device Robot Vision</code>.</li> </ul> <p>Save the detect objects job in the device projects folder of the Machine Vision Device.</p>"},{"location":"4_0_robot_vision_server/4_5_0_generic_robots/","title":"4.5 Generic Robots","text":"<p>Select the robot manufacturer <code>Generic</code> on the device website (tab <code>Jobs</code> -&gt; <code>Robot Server</code>, see section 4.2 Settings on Device Website) in order to use the generic string based robot vision API (see section 4.6 Generic Robot Vision API).</p> <p>NOTE:</p> <p>The python robot example for the generic string based robot vision API is available in the related GitHub Repository</p>"},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/","title":"4.6 Generic Robot Vision API","text":"<p>The generic robot vision API describes the communication between the robot controller and the wenglor robot server on the Machine Vision Device.</p> <p>Overview of commands for string and XML based robots:</p>"},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/#command-syntax","title":"Command syntax","text":"Command Description Unit/Note job:change[job_name.u3p]; Changes the uniVision job of the processing instance to the given job. File ending \u201c.u3p\u201d is mandatory. Don't wrap the job name in quotes job:get; Requests the currently active job of the processing instance. String calibration:clear; Clears the internal buffer of the robot server. \u00a0Required in case of recalibrating. calibration:add[pose_information]; Triggers and saves the current camera image together with the given robot pose in the internal robot server buffer. Pose information in [x, y, z, rx, ry, rz] x, y, z in meter rx, ry, rz as rotation vector in radiant calibration:calculate[calibration_case, calibration_target]; Calculates the hand-eye calibration based on the saved calibration poses and images from the internal robot server. The results are stored on the Machine Vision Device. Calibration case options: <ul><li>camera_on_robot</li><li>camera_not_on_robot </li></ul> Calibration target options: <ul> <li>ZVZJ001</li> <li>ZVZJ002</li> <li>ZVZJ003</li> <li>ZVZJ004</li></ul>  NOTEFor ZVZJ005 use ZVZJ001 For\u00a0ZVZJ006 use ZVZJ002 (same size)  calibration:ground[calibration_target]; Triggers the uniVision job, calculates and saves the current calibration target pose as an object ground reference. Calibration target options: <ul> <li>ZVZJ001</li> <li>ZVZJ002</li> <li>ZVZJ003</li> <li>ZVZJ004</li></ul>  NOTEFor ZVZJ005 use ZVZJ001 For\u00a0ZVZJ006 use ZVZJ002 (same size)  state[calibration_case]; Requests the state of the robot server. The first bit represents the overall state. The second bit represents if a calibration is present for the provided use case or not. Calibration case options: <ul><li>camera_on_robot </li> <li>camera_not_on_robot </li></ul> First bit: Connection error state <ul> <li>0: No camera error present </li> <li>1: Camera is in error state </li></ul> Second bit: Calibration state <ul><li>0: No calibration present </li> <li>1: Calibration data could be read </li></ul> Example: 01 <ul><li>0: No camera error </li> <li>1: Calibration available </li></ul> Example: 10 <ul><li>1: Camera in error state </li> <li>0: Calibration not available </li></ul> Example: 11 <ul> <li>1: Camera in error state </li> <li>1: Calibration available detect[calibration_case, pose_information]; Triggers the uniVision job, sends the <code>Device Robot Vision</code> data to the robot server. There the 3D object pose is calculated based on the current calibration data and returned to the robot. Calibration case options: <ul><li>camera_on_robot</li> <li>camera_not_on_robot </li></ul>Pose information in [x, y, z, rx, ry, rz]  x, y, z in meter  rx, ry, rz as rotation vector in radiant  Object pose (array of floating point numbers) num_objects:get; Requests the number of objects available in the robot server buffer Positive number validate[calibration_case, detection_pose]; Provides the calibration target origin 3D pose (bottom left target) based on the information saved during the calibration. Calibration case options: <ul><li>camera_on_robot </li> <li>camera_not_on_robot </li></ul> Pose information in [x, y, z, rx, ry, rz]  x, y, z in meter  rx, ry, rz as rotation vector in radiant  Object pose (array of floating point numbers) pose:get[index]; Requests the object pose with the given index from the robot server buffer. Detect must be called first to fill the buffer. Index starts at 0  Pose information in [x, y, z, rx, ry, rz]  x, y, z in meter  rx, ry, rz as rotation vector in radiant  Object pose (array of floating point numbers) shape:get[index]; Requests the shape model with the given index from the robot server buffer. Detect must be called first to fill the buffer. Index starts at 0  Number value:get[index]; Requests the additional value with the given index from the robot server buffer. Detect must be called first to fill the buffer. Index starts at 0  String"},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/#return-values","title":"Return values","text":"Command Successful return value(for string based robots) Error return value (for string based robots) job:change[job_name.u3p]; 0 Error code as negative number job:get; uniVision job name [string] Error code as negative number calibration:clear; 0 Error code as negative number calibration:add[pose_information]; 0 Error code as negative number calibration:calculate[calibration_case,calibration_target]; reprojection error positive floating point Error code as negative number calibration:ground[calibration_target]; 0 Error code as negative number state[calibration_case]; Two binary numbers <ul><li>First bit: Camera error</li> <li>Second bit: Calibration state</li></ul> Two binary numbers <ul><li>First bit: Camera error</li> <li>Second bit: Calibration state</li></ul> detect[calibration_case,pose_information]; Object pose (array of floating point numbers) Error code as negative number num_objects:get; Number of objects available in robot server (positive number) Error code as negative number validate[calibration_case,detection_pose]; Calibration target pose (array of floating point numbers) Error code as negative number pose:get[index]; Object pose to the object with the corresponding index (starting with 0) (Array of floating point numbers) Error code as negative number shape:get[index]; Shape model linked in <code>Device Robot Vision</code> to the object with the corresponding index (starting with 0) (number) Error code as negative number value:get[index]; Additional value linked in <code>Device Robot Vision</code> to the object with the corresponding index (starting with 0)(string) Error code as negative number"},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/#error-codes","title":"Error codes","text":"Code number Error message Notes -5001 General Error If an error was recognized, but could not be assigned to any other error code. -5002 Badly formatted request <ul><li>If the extraction of the parameter failed.</li> <li>The command ending is missing.</li><li>The command is unknown.</li></ul> -5003 No connnection to univision <ul><li>The required connection to the processing instance is not available.</li> <li>The required connection to <code>Device Robot Vision</code> is not available.</li></ul> -5004 Unknown univision job name If the job could not be loaded, but for a different reason than a connection issue. -5005 Badly configured univision job If the number of shape models of locator/pattern match does not fit to the configuration of <code>Device Robot Vision</code>. -5006 Calibration failed If the calibration failed for any reason. -5007 No calibration data If the required calibration data is not available to calculate the 3d object pose. -5008 No object found If <code>Device Robot Vision</code> sends result true count = 0. -5009 Bad or empty <code>Device Robot Vision</code> message\" If the extraction of <code>Device Robot Vision</code> message fails, mostly the case if it is empty. -5010 Index error If the user tries to access data that is out of range, e.g.num_objects:get; returned 1 object was found, and you are trying to access shape:get[1];. As the index starts counting at 0, the user would try to access object 2,while only one was found."},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/#example-xml-reply","title":"Example XML reply","text":"<pre><code>&lt;sensor&gt;\n &lt;reply&gt;\n &lt;command&gt;job:get;&lt;/command&gt;\n &lt;return_code&gt;0&lt;/return_code&gt;\n &lt;job_name&gt;calibration.u3p&lt;/job_name&gt;\n &lt;calib_accuracy&gt;0&lt;/calib_accuracy&gt;\n &lt;state&gt;00&lt;/state&gt;\n &lt;object_pose X=\"0\" Y=\"0\" Z=\"0\" RX=\"0\" RY=\"0\" RZ=\"0\"/&gt;\n &lt;num_objects&gt;0&lt;/num_objects&gt;\n &lt;shape_model&gt;0&lt;/shape_model&gt;\n &lt;additional_value&gt;&lt;/additional_value&gt;\n &lt;/reply&gt;\n&lt;/sensor&gt;\n</code></pre>"},{"location":"4_0_robot_vision_server/4_6_0_generic_robot_vision_api/#example-string-replies","title":"Example string replies","text":"Command from robot Successful reply from robot server job:change[calibration.u3p]; 0 job:get; calibration.u3p calibration:clear; 0 calibration:add[[0.021476,-1.344395,0.716645,-3.080280,-0.024544,-0.476923]]; 0 calibration:calculate[camera_not_on_robot,zvzj004]; 0.129936 calibration:ground[zvzj004]; 0 state[camera_not_on_robot]; 01 detect[camera_not_on_robot,[0.047871,-0.856617,0.830479,0.874365,3.004280,-0.045222]]; (0.048705,-0.094640,-0.110581,0.102101,-3.131079,0.003192) validate[camera_not_on_robot, [0.047871,-0.856617,0.830479,0.874365,3.004280,-0.045222]]; (0.318205,-0.164630,-0.100281,0.090210,-1.131079,1.003192) num_objects:get; 5 pose:get[0]; (0.048705,-0.094640,-0.110581,0.102101,-3.131079,0.003192) shape:get[1]; 0 value:get[4]; 0.903153 <p>NOTE</p> <p>An example robot program structure written in Python that shows how to use the generic robot  vision API is available in the related GitHub Repository</p>"},{"location":"4_0_robot_vision_server/4_1_basics/","title":"4.1 Basics with Robot Server","text":"<p>The calibration process is different, if the camera is mounted on the robot or not on the robot. Buy one of the calibration plates ZVZJ (see wenglor website -&gt; ZVZJ) or print the PDF on a stiff and flat material (see wenglor website -&gt; ZVZJ). For best accuracy, use the wenglor calibration plates ZVZJ.</p> <p>NOTE</p> <ul> <li>In case of printing, make sure to print the PDFs at actual size.</li> <li>Typically, the reprojection error for the ZVZJ calibration plate is five times smaller compared to the printed version.</li> <li>For direct light applications, non-transparent calibration plates made of carbon fiber are available. For backlight applications, transparent calibration plates with the material glass are available.</li> <li>The calibration plate should cover at least half of the image and should be visible completely by the camera if possible for most accurate results.</li> </ul> <p>Consider the following points when setting the calibration poses:</p> <ul> <li>Check that the camera sees the calibration plate when setting the poses.</li> <li>Make sure that the difference between one pose and the next one is as big as possible for best accurate results. The same pose could be used several times if not consecutive (e.g. pose one and three can be similar). If space is limited in the application, small differences between one pose and the next one are also possible, but result in less accurate results.</li> <li>The calibration plate should cover as much as possible of the camera image and should be visible completely if possible for most accurate results.</li> </ul>"},{"location":"4_0_robot_vision_server/4_1_basics/4_1_1_camera_on_robot/","title":"4.1.1 Camera on Robot","text":"<p>If the camera is on the robot, teach minimum five calibration poses. For more accurate results, teach further calibration poses (e.g. seven to eleven poses).</p> <p>The first calibration pose is also the detection pose. Make sure to use a suitable pose where the objects are located in a safe way. Make sure to use big variations between the different poses.</p> Calibration pose 1 Calibration pose 2 Calibration pose 3 Calibration pose 4 Calibration pose 5 <p>After calibration, an optional verification step can be performed to check its accuracy. Applying it, moves the robot TCP to the bottom left corner of the calibration plate (with an adjustable safety height offset). It is necessary that the calibration plate was not moved between the calibration and the verification step. In case of bad results, check the setup and re-run the calibration.</p> <p>NOTE</p> <ul> <li>For the verification step, the Z-axis must point to the object plane.</li> <li>The reprojection error shows how good the calibration was. Typical values are 0.1 for ZVZJ calibration plates and 0.5 for printed calibration plates.</li> </ul> <p></p> <p>After successful calibration, pick your objects. For the detection pose, it is mandatory to use the position of the first calibration pose.</p> <p></p> <p>With the object position sent by the camera, the robot moves to the object pose.</p> <p></p>"},{"location":"4_0_robot_vision_server/4_1_basics/4_1_2_camera_not_on_robot/","title":"4.1.2 Camera not on Robot","text":"<p>If the camera is not on the robot, the calibration process consists of two steps.</p> <p>For the first step of the calibration process, mount the calibration plate on the robot. Teach minimum five calibration poses (e.g. seven to eleven poses). For more accurate results, teach further calibration poses.</p> Calibration pose 1 Calibration pose 2 Calibration pose 3 Calibration pose 4 Calibration pose 5 <p>For the second step of the calibration process, put the calibration plate on the measuring or picking plane and capture one single image.</p> <p></p> <p>After running the calibration, an optional verification step is possible to check the accuracy of the calibration. Applying it, moves the robot TCP to the bottom left corner of the calibration plate (with an adjustable safety height offset). It is necessary that the calibration plate was not moved between the second calibration step and the verification step. In case of bad results, check the setup and re-run the calibration.</p> <p>NOTE</p> <ul> <li>For the verification step, the z axis must point to the object plane. In tilted settings, the Z-axis of the TCP will be perpendicular to the object plane.</li> <li>The reprojection error shows how good the calibration was. Typical values are 0.1 for ZVZJ calibration plates and 0.5 for printed calibration plates.</li> </ul> <p>After successful calibration, pick your objects. Use any detection pose where the camera sees the objects.</p> <p></p> <p>With the object position sent by the camera, move the robot to the object.</p> <p></p>"}]}